{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQJNKoDNYkui"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "foat_nu8Yxfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(json_data):\n",
        "    prepared_data = []\n",
        "    for item in json_data:\n",
        "        input_text = f\"Customer: {item['customer']}\\nSales Rep Response:\"\n",
        "        target_text = item['sales_rep']\n",
        "        prepared_data.append(f\"{input_text} {target_text}\")\n",
        "    return prepared_data"
      ],
      "metadata": {
        "id": "ue_rfTIBY3KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('genotek.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "prepared_data = prepare_data(data)"
      ],
      "metadata": {
        "id": "B7_WJaQ9Y_sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zuY4ny2PZO-Z",
        "outputId": "a2f85da7-676d-40f4-c655-6a2709398481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Customer: Your delivery times are longer than other companies. Why should I wait?\\nSales Rep Response: I understand your concern about delivery times. Could you tell me about your typical order volumes and frequency? This will help me explain how our shipping process might actually save you time in the long run.',\n",
              " \"Customer: I've heard your customer service isn't very responsive. How can you assure me I won't be left hanging?\\nSales Rep Response: I appreciate you bringing up this concern. Can you share what specific aspects of customer service are most critical for your business? This will help me highlight how our support system aligns with your needs.\",\n",
              " \"Customer: Your software seems complicated. I'm worried my staff won't be able to use it effectively.\\nSales Rep Response: That's a valid concern. Could you tell me more about your team's experience with similar systems? This will help me suggest the most appropriate training program and estimate the learning curve.\",\n",
              " \"Customer: Your competitors offer more customization options. Why should I choose your more rigid system?\\nSales Rep Response: Thank you for raising this point. Could you elaborate on the specific customizations you're looking for? This will help me explain how our system might actually meet those needs without requiring extensive customization.\",\n",
              " 'Customer: Your product has fewer features than others in the market. How can you justify the price?\\nSales Rep Response: I understand your concern about features versus price. Can you share which features are most crucial for your operations? This will help me demonstrate how our streamlined approach might actually be more efficient for your needs.',\n",
              " \"Customer: I'm worried about the integration process with our existing systems. How complicated will this be?\\nSales Rep Response: That's an important consideration. Could you provide more details about your current systems and any past integration challenges? This will help me outline a potential integration plan tailored to your situation.\",\n",
              " \"Customer: Your company is smaller than some of your competitors. How can I trust you'll be around long-term?\\nSales Rep Response: I appreciate your concern about longevity. Can you share what aspects of a long-term partnership are most important to you? This will help me explain how our company's structure and philosophy align with those needs.\",\n",
              " \"Customer: I've seen mixed reviews about your product online. Why should I believe the positive ones?\\nSales Rep Response: Thank you for your honesty about the reviews. Could you tell me which specific aspects of the product these reviews mentioned that concern you? This will help me address those points directly and provide relevant case studies.\",\n",
              " \"Customer: Your contract terms seem more restrictive than others. Why can't you be more flexible?\\nSales Rep Response: I understand your concern about contract flexibility. Can you highlight which terms specifically feel restrictive to you? This will help me explain the reasoning behind those terms or explore potential adjustments.\",\n",
              " \"Customer: I'm not sure if your product will scale with our business growth. How can you assure me it will?\\nSales Rep Response: That's a crucial point about scalability. Could you share your projected growth plans and key milestones? This will help me demonstrate how our product is designed to accommodate and support business expansion.\",\n",
              " \"Customer: Your onboarding process seems lengthy. Why can't we get started faster?\\nSales Rep Response: I understand your desire for a quick start. Can you tell me more about your ideal timeline and any specific deadlines you're working with? This will help me explain how our onboarding process might actually save time in the long run or explore ways to expedite it.\",\n",
              " \"Customer: I'm concerned about data security with your cloud-based solution. How can you guarantee our information will be safe?\\nSales Rep Response: Data security is indeed crucial. Could you share any specific security protocols or compliance requirements your company follows? This will help me outline how our security measures align with or exceed those standards.\",\n",
              " \"Customer: Your product updates seem less frequent than competitors. How can I be sure I'm getting the latest technology?\\nSales Rep Response: I appreciate your focus on staying current. Can you tell me which types of updates or new features are most important for your business? This will help me explain our update philosophy and how it ensures stability while incorporating meaningful improvements.\",\n",
              " \"Customer: I'm worried about vendor lock-in with your proprietary system. What if we need to switch later?\\nSales Rep Response: That's a valid concern about future flexibility. Could you share more about your long-term technology strategy? This will help me explain how our system is designed for interoperability and data portability, potentially alleviating your lock-in concerns.\",\n",
              " \"Customer: Your pricing model is different from what we're used to. Why should we change our budgeting approach?\\nSales Rep Response: I understand your hesitation about a new pricing model. Can you walk me through your current budgeting process and any pain points you've experienced? This will help me demonstrate how our pricing model might actually simplify your budgeting and potentially save costs in the long run.\",\n",
              " \"Customer: Your product seems to have a steep learning curve. How can I ensure my team will adapt quickly?\\nSales Rep Response: I understand your concern about the learning curve. Can you share more about your team's technical proficiency and past training experiences? This will help me tailor our training resources to fit your needs.\",\n",
              " \"Customer: I'm concerned about the environmental impact of your product. How sustainable is it?\\nSales Rep Response: Thank you for raising this important issue. Can you provide details on your sustainability goals and criteria? This will help me demonstrate how our product aligns with or supports your environmental initiatives.\",\n",
              " 'Customer: Your competitors have a broader global presence. How can you support our international operations?\\nSales Rep Response: I appreciate your concern about global support. Could you tell me more about your international locations and specific needs? This will help me explain how our global partnerships and support infrastructure can assist you.',\n",
              " \"Customer: I've heard about potential hidden costs with your service. How can I be sure there won't be surprises?\\nSales Rep Response: It's important to address cost transparency. Can you share specific concerns or experiences with hidden costs? This will help me clarify our pricing structure and highlight our commitment to transparency.\",\n",
              " 'Customer: Your product seems to lack integration with some of our key tools. How can this be resolved?\\nSales Rep Response: Integration is crucial. Could you list the specific tools you use and any integration needs? This will help me discuss how we can facilitate seamless integration with our solution.',\n",
              " \"Customer: Your website doesn't provide enough detailed information. How can I get a better understanding of your product?\\nSales Rep Response: I appreciate your need for detailed information. Can you specify which areas you're most interested in? This will help me provide the relevant resources and arrange a detailed demonstration.\",\n",
              " \"Customer: Your support hours don't align with our business operations. How can we ensure timely assistance?\\nSales Rep Response: Support availability is vital. Could you outline your operating hours and critical times for support? This will help me discuss potential solutions to ensure you receive timely assistance.\",\n",
              " \"Customer: I've seen better analytics features in other products. How does yours compare?\\nSales Rep Response: Analytics is a key aspect. Can you describe the specific analytics features you need? This will help me highlight how our analytics capabilities meet or exceed those requirements.\",\n",
              " \"Customer: Your trial period seems short. How can I get a better sense of the product's capabilities?\\nSales Rep Response: A thorough evaluation is important. Could you share your typical evaluation process and timeline? This will help me discuss options to extend the trial or provide additional resources.\",\n",
              " \"Customer: I'm not sure if your solution is compatible with our industry regulations. Can you assure compliance?\\nSales Rep Response: Regulatory compliance is critical. Could you specify the regulations and standards you must adhere to? This will help me demonstrate how our solution meets those compliance requirements.\",\n",
              " \"Customer: Your references are from smaller companies. How can I be sure your product scales for larger enterprises?\\nSales Rep Response: Scaling for larger enterprises is important. Can you describe your company's size and specific scalability needs? This will help me share relevant case studies and examples.\",\n",
              " 'Customer: Your documentation seems less comprehensive than others. How can we ensure we have the support we need?\\nSales Rep Response: Comprehensive documentation is crucial. Could you specify what areas you need more information on? This will help me provide additional resources and highlight our support channels.',\n",
              " 'Customer: Your maintenance costs appear higher. How can we justify this expense?\\nSales Rep Response: Maintenance costs are an important factor. Can you share your current maintenance costs and challenges? This will help me explain how our maintenance plan offers long-term value.',\n",
              " \"Customer: Your product lifecycle seems shorter than competitors. How can we plan for long-term use?\\nSales Rep Response: Long-term planning is critical. Can you describe your typical product lifecycle management process? This will help me demonstrate our product's longevity and future-proofing measures.\",\n",
              " 'Customer: Your upgrade process appears complex. How can we ensure seamless updates?\\nSales Rep Response: A smooth upgrade process is essential. Can you share past experiences with complex upgrades? This will help me outline our streamlined upgrade procedures.',\n",
              " \"Customer: Your product's offline capabilities seem limited. How can we operate in low-connectivity environments?\\nSales Rep Response: Operating offline is important for some businesses. Could you describe the specific offline requirements you have? This will help me explain how our product supports low-connectivity scenarios.\",\n",
              " 'Customer: Your user interface seems less intuitive. How can we ensure user adoption?\\nSales Rep Response: A user-friendly interface is essential. Can you share what specific aspects of the interface are concerning? This will help me highlight our usability features and available training.',\n",
              " \"Customer: I'm worried about the transition period disrupting our operations. How can this be minimized?\\nSales Rep Response: Minimizing disruption is key. Could you detail your current processes and potential pain points during transition? This will help me outline a smooth transition plan tailored to your operations.\",\n",
              " 'Customer: Your service level agreements (SLAs) seem less robust than others. How can you ensure reliability?\\nSales Rep Response: I understand the importance of strong SLAs. Could you outline the critical service levels you expect? This will help me explain our SLAs and how they guarantee reliable service.']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('prepared_data.txt', 'w') as f:\n",
        "    for item in prepared_data:\n",
        "        f.write(f\"{item}\\n\")"
      ],
      "metadata": {
        "id": "9IDUctmVZQsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"prepared_data.txt\",\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6nDnnbXZVg9",
        "outputId": "05fd4bfa-3462-492a-f45d-62207060fdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "ciyEMCHkZbTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0UoRej-KZ7Qf",
        "outputId": "de5f3ebe-26a0-4937-f20f-c221ac228b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cMiSUTJKadeb",
        "outputId": "9aeeacd1-ac13-43d6-9ec7-e66fa7ff1e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers accelerate torch\n",
        "!pip install transformers[torch] accelerate torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "rTYWr-5tatBq",
        "outputId": "1afa1daf-e27f-494a-a5c8-6e82e2dd6f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.42.4\n",
            "Uninstalling transformers-4.42.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.42.4.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled transformers-4.42.4\n",
            "Found existing installation: accelerate 0.32.1\n",
            "Uninstalling accelerate-0.32.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/accelerate\n",
            "    /usr/local/bin/accelerate-config\n",
            "    /usr/local/bin/accelerate-estimate-memory\n",
            "    /usr/local/bin/accelerate-launch\n",
            "    /usr/local/bin/accelerate-merge-weights\n",
            "    /usr/local/lib/python3.10/dist-packages/accelerate-0.32.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/accelerate/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled accelerate-0.32.1\n",
            "Found existing installation: torch 2.3.1\n",
            "Uninstalling torch-2.3.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-2.3.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torch-2.3.1\n",
            "Collecting transformers[torch]\n",
            "  Using cached transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch, transformers, accelerate\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.1 torch-2.3.1 transformers-4.42.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate",
                  "torch",
                  "torchgen",
                  "transformers"
                ]
              },
              "id": "ccb35282fb03448fb7b55ea0b6cfb1b0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=60,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "tj4OSdnzZiYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "gkxq8rdlZmuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "xxwe_qdKfmR6",
        "outputId": "a29eecbb-8c88-441c-a498-9c3b4b4ff02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 16:50, Epoch 60/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=120, training_loss=0.040737239519755046, metrics={'train_runtime': 1022.8358, 'train_samples_per_second': 0.411, 'train_steps_per_second': 0.117, 'total_flos': 27435663360000.0, 'train_loss': 0.040737239519755046, 'epoch': 60.0})"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaQv94UOfqco",
        "outputId": "d1c6723d-832f-4c8f-df68-823ce164c6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_gpt2/tokenizer_config.json',\n",
              " './fine_tuned_gpt2/special_tokens_map.json',\n",
              " './fine_tuned_gpt2/vocab.json',\n",
              " './fine_tuned_gpt2/merges.txt',\n",
              " './fine_tuned_gpt2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-2w2CfgJ_6",
        "outputId": "f7484913-a925-45d4-9eef-32ac0471fc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "o5U2iUWtgs9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./fine_tuned_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path, pad_token='<|endoftext|>')\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "fnjdtnQdhpHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_with_examples(customer_objection):\n",
        "    prompt = f\"\"\"\n",
        "Here are two examples of good sales representative responses:\n",
        "\n",
        "Customer: Your product is more expensive than competitors. Why should I pay more?\n",
        "Sales Rep:\n",
        "1. Acknowledgment: I understand your concern about our pricing.\n",
        "2. Question: Could you tell me which specific features are most important for your business?\n",
        "3. Value proposition: Our higher price reflects superior quality and advanced features that often lead to greater long-term cost savings and efficiency for our clients.\n",
        "\n",
        "Customer: I'm worried about the learning curve for my team.\n",
        "Sales Rep:\n",
        "1. Acknowledgment: It's natural to be concerned about the learning process for new software.\n",
        "2. Question: Can you tell me about your team's experience with similar systems?\n",
        "3. Value proposition: We offer comprehensive training and ongoing support to ensure a smooth transition and quick proficiency for your team.\n",
        "\n",
        "Now, please respond to this customer objection:\n",
        "Customer: {customer_objection}\n",
        "\n",
        "Sales Representative's Response:\n",
        "\"\"\"\n",
        "def generate_response(customer_objection, max_length=150):\n",
        "    input_text = f\"Customer: {customer_objection}\\nSales Rep Response:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            temperature=0.4\n",
        "        )\n",
        "\n",
        "    response1 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    response1 = response1.split(\"Sales Rep Response:\")[1].strip()\n",
        "    first_line = response1.split('\\n')[0].strip()\n",
        "    return first_line\n",
        "\n",
        "test_objection = \"Your training resources seem limited compared to others. How can I ensure my team will be fully prepared?\"\n",
        "response1 = generate_response(test_objection)\n",
        "print(f\"Generated response: {response1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biOtNsDGwin0",
        "outputId": "ef1086b2-41cb-4ef5-ef93-149eb3384db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: I understand your concern about training staff. Can you share which types of resources are most crucial for your operations? This will help me demonstrate how our onboarding process might actually save time in the long run or explore ways to expedite it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID63vS8w6PzZ",
        "outputId": "53c7e84e-bb53-49e2-a889-4d774fede172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "Fv8V49oE664z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(customer_objection, max_length=150):\n",
        "    input_text = f\"Customer: {customer_objection}\\nSales Rep Response:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            temperature=0.4\n",
        "        )\n",
        "\n",
        "    response1 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    response1 = response1.split(\"Sales Rep Response:\")[1].strip()\n",
        "    first_line = response1.split('\\n')[0].strip()\n",
        "    return first_line\n",
        "\n",
        "customer_concerns = [\n",
        "    \"I've heard that your product can be difficult to customize for unique business needs. How flexible is it?\",\n",
        "    \"Your product seems to lack robust reporting features. How can I ensure it meets our reporting needs?\",\n",
        "    \"I'm worried about the reliability of your product during peak usage times. How do you ensure consistent performance?\",\n",
        "    \"Your product's mobile functionality seems limited. How can I be sure it will work well for our mobile workforce?\",\n",
        "    \"I've noticed your product has fewer integrations with third-party apps. How can I ensure it fits into our existing tech ecosystem?\"\n",
        "]\n",
        "\n",
        "responses = [generate_response(concern) for concern in customer_concerns]\n",
        "\n",
        "table_data = []\n",
        "for i, (concern, response) in enumerate(zip(customer_concerns, responses), start=1):\n",
        "    table_data.append([f\"Concern {i}\", concern, response])\n",
        "\n",
        "# Define table headers\n",
        "headers = [\"#\", \"Customer Concern\", \"Sales Rep Response\"]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWN3wTpx4ING",
        "outputId": "cad42bab-4dac-4c5d-95a2-fa062e15060f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| #         | Customer Concern                                                                                                                   | Sales Rep Response                                                                                                                                                                                                                                                                                      |\n",
            "+===========+====================================================================================================================================+=========================================================================================================================================================================================================================================================================================================+\n",
            "| Concern 1 | I've heard that your product can be difficult to customize for unique business needs. How flexible is it?                          | That's a valid concern about customization. Could you tell me more about your ideal product line-up and any specific deadlines you're working with? This will help me explain how our product might actually meet those needs without requiring extensive customization or extensive product redesigns. |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Concern 2 | Your product seems to lack robust reporting features. How can I ensure it meets our reporting needs?                               | I understand your concern about robustness. Can you share what aspects of a robust system are most important to you? This will help me demonstrate how our integration process might actually save time in the long run or explore ways to expedite it.                                                 |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Concern 3 | I'm worried about the reliability of your product during peak usage times. How do you ensure consistent performance?               | That's a crucial point about reliability. Could you tell me more about your typical order volumes and frequency? This will help me suggest the most appropriate shipping method and estimate the shipping timeframe.                                                                                    |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Concern 4 | Your product's mobile functionality seems limited. How can I be sure it will work well for our mobile workforce?                   | I understand your concern about accessibility. Can you share which types of features are most crucial for your operations? This will help me demonstrate how our streamlined approach might actually be more efficient for you.                                                                         |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Concern 5 | I've noticed your product has fewer integrations with third-party apps. How can I ensure it fits into our existing tech ecosystem? | That's a valid concern. Can you share which types of integrators or partners are most likely to be successful with your new product? This will help me demonstrate how our streamlined approach might actually be more efficient for your needs.                                                        |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZRCticTaCT3T",
        "outputId": "bdee3077-91fa-48e3-bbe8-e8e44d8d7ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the models and tokenizers\n",
        "base_model_name = \"gpt2\"\n",
        "fine_tuned_model_path = \"./fine_tuned_gpt2\"\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Set pad_token to eos_token if pad_token is not already set\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "if fine_tuned_tokenizer.pad_token is None:\n",
        "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n"
      ],
      "metadata": {
        "id": "HGBRwgaBFfhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_responses(prompts, tokenizer, model, max_length=50):\n",
        "    responses = []\n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=2,\n",
        "                temperature=0.7\n",
        "            )\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        responses.append(response)\n",
        "    return responses\n",
        "\n",
        "# Example test prompts\n",
        "test_prompts = [\n",
        "    \"I've heard that customizing your product for specific business needs can be challenging. How adaptable is it?\",\n",
        "    \"Iâ€™m concerned about how reliable your product is during high-demand periods. What measures are in place to maintain performance?\",\n",
        "    \"It seems like your product may not have strong reporting capabilities. How can you ensure it covers all our reporting needs?\",\n",
        "    \"Your productâ€™s mobile features seem a bit limited. How can you assure me that itâ€™s suitable for a mobile workforce?\",\n",
        "    \"I noticed your product integrates with fewer third-party applications. How can I ensure it fits with the tools we already use?\"\n",
        "]\n",
        "\n",
        "# Generate responses from both models\n",
        "base_responses = generate_responses(test_prompts, base_tokenizer, base_model)\n",
        "fine_tuned_responses = generate_responses(test_prompts, fine_tuned_tokenizer, fine_tuned_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwMl3yvQGmDQ",
        "outputId": "07ff4a3f-bc5c-4173-b704-8b22e92705d9"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXoNXk0cGq7m",
        "outputId": "3f992154-6f68-4171-ab4e-890152065f5f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I've heard that customizing your product for specific business needs can be challenging. How adaptable is it?\\n\\nI think it's a good question. I think you can do a lot of things with your custom product. You can make it\", \"Iâ€™m concerned about how reliable your product is during high-demand periods. What measures are in place to maintain performance?\\n\\nI'm concerned that the product's performance is not being maintained. I'm also concerned with the fact that I\", 'It seems like your product may not have strong reporting capabilities. How can you ensure it covers all our reporting needs?\\n\\nWe have a lot of reporting requirements that we need to meet. We have to be able to report on the most important issues', 'Your productâ€™s mobile features seem a bit limited. How can you assure me that itâ€™s suitable for a mobile workforce?\\n\\nWe are currently working on a solution to this problem. We are working with our partners to provide a', 'I noticed your product integrates with fewer third-party applications. How can I ensure it fits with the tools we already use?\\n\\nWe have a lot of tools that we use to help us integrate our products with our customers. We have built a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_tuned_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWYzJEhhHQSA",
        "outputId": "1d63683c-1df5-4230-af9a-2f0003b86f0d"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I've heard that customizing your product for specific business needs can be challenging. How adaptable is it?\\nSales Rep Response: I appreciate your concern about customizations. Can you share what specific aspects of customization are most critical for your operations\", 'It seems like your product may not have strong reporting capabilities. How can you ensure it covers all our reporting needs?\\nSales Rep Response: I appreciate your concern about our ability to meet our needs. Can you share what specific aspects of our onboarding', 'Iâ€™m concerned about how reliable your product is during high-demand periods. What measures are in place to maintain performance?\\nSales Rep Response: I appreciate your concern about reliability. Can you share which types of adjustments are most important for your', 'Your productâ€™s mobile features seem a bit limited. How can you assure me that itâ€™s suitable for a mobile workforce?\\nSales Rep Response: I appreciate your concern about accessibility. Can you share what aspects of a streamlined approach are', 'I noticed your product integrates with fewer third-party applications. How can I ensure it fits with the tools we already use?\\nSales Rep Response: I appreciate your focus on simplicity. Can you tell me which types of applications or frameworks are most important']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(prompts, tokenizer, model):\n",
        "    total_log_likelihood = 0\n",
        "    total_tokens = 0\n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs.input_ids)\n",
        "        log_likelihood = -outputs.loss.item()\n",
        "        total_log_likelihood += log_likelihood\n",
        "        total_tokens += len(inputs.input_ids[0])\n",
        "    average_log_likelihood = total_log_likelihood / len(prompts)\n",
        "    perplexity = torch.exp(torch.tensor(average_log_likelihood))\n",
        "    return perplexity.item()\n",
        "\n",
        "# Calculate perplexity for both models\n",
        "base_perplexity = calculate_perplexity(test_prompts, base_tokenizer, base_model)\n",
        "fine_tuned_perplexity = calculate_perplexity(test_prompts, fine_tuned_tokenizer, fine_tuned_model)\n",
        "\n",
        "print(f\"Base GPT-2 Perplexity: {base_perplexity}\")\n",
        "print(f\"Fine-Tuned Model Perplexity: {fine_tuned_perplexity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g92S7SMOHqsQ",
        "outputId": "e50517e5-caed-4d49-cf6f-4f5bfef8a96a"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base GPT-2 Perplexity: 0.015939459204673767\n",
            "Fine-Tuned Model Perplexity: 0.004965505562722683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJh52s86JQUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}